{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2c94951-b4ca-46a9-88fd-1cacbe61bc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\omarm\\anaconda3\\envs\\projet\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in c:\\users\\omarm\\anaconda3\\envs\\projet\\lib\\site-packages (from pyspark) (0.10.9.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea58b8f-acdf-4c5c-9c9f-0e43e01bdcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sch√©ma final compatible SQL :\n",
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY_OF_MONTH: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- OP_UNIQUE_CARRIER: string (nullable = true)\n",
      " |-- ORIGIN_CITY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_STATE_ABR: string (nullable = true)\n",
      " |-- DEST_CITY_NAME: string (nullable = true)\n",
      " |-- DEST_STATE_ABR: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: string (nullable = true)\n",
      " |-- DEP_DELAY_NEW: string (nullable = true)\n",
      " |-- CRS_ARR_TIME: string (nullable = true)\n",
      " |-- ARR_DELAY_NEW: string (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_CODE: string (nullable = true)\n",
      " |-- AIR_TIME: string (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      "\n",
      "+----+-----+------------+-----------+-----------------+----------------+----------------+--------------+--------------+------------+-------------+------------+-------------+---------+-----------------+--------+--------+\n",
      "|YEAR|MONTH|DAY_OF_MONTH|DAY_OF_WEEK|OP_UNIQUE_CARRIER|ORIGIN_CITY_NAME|ORIGIN_STATE_ABR|DEST_CITY_NAME|DEST_STATE_ABR|CRS_DEP_TIME|DEP_DELAY_NEW|CRS_ARR_TIME|ARR_DELAY_NEW|CANCELLED|CANCELLATION_CODE|AIR_TIME|DISTANCE|\n",
      "+----+-----+------------+-----------+-----------------+----------------+----------------+--------------+--------------+------------+-------------+------------+-------------+---------+-----------------+--------+--------+\n",
      "|2020|    1|          17|          5|               WN|       Las Vegas|              NV|       Chicago|            IL|    08:45:00|           24|    14:10:00|            8|        0|             NULL|     171|    1521|\n",
      "|2020|    1|          17|          5|               WN|       Las Vegas|              NV|       Chicago|            IL|    05:50:00|            2|    11:10:00|            0|        0|             NULL|     172|    1521|\n",
      "|2020|    1|          17|          5|               WN|       Las Vegas|              NV|       Chicago|            IL|    13:55:00|         NULL|    19:30:00|         NULL|        1|                B|    NULL|    1521|\n",
      "|2020|    1|          17|          5|               WN|       Las Vegas|              NV|       Chicago|            IL|    16:40:00|         NULL|    22:05:00|         NULL|        1|                B|    NULL|    1521|\n",
      "|2020|    1|          17|          5|               WN|       Las Vegas|              NV|       Chicago|            IL|    07:40:00|            0|    13:05:00|            0|        0|             NULL|     181|    1521|\n",
      "+----+-----+------------+-----------+-----------------+----------------+----------------+--------------+--------------+------------+-------------+------------+-------------+---------+-----------------+--------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialiser la session Spark \n",
    "spark = SparkSession.builder.appName(\"ImportForSQLPrep\").getOrCreate()\n",
    "\n",
    "# 1. Charger le fichier CSV \n",
    "# Spark lira les donn√©es et attribuera des noms de colonnes g√©n√©riques (_c0, _c1, ...)\n",
    "df_spark = spark.read.csv(\"flights_2020.csv\", header=False, inferSchema=True)\n",
    "\n",
    "# 2. D√©finir les noms de colonnes propres et compatibles SQL\n",
    "# Ces noms sont ceux que nous utiliserons ensuite dans nos requ√™tes SQL\n",
    "sql_compliant_column_names = [\n",
    "    'YEAR', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'OP_UNIQUE_CARRIER', \n",
    "    'ORIGIN_CITY_NAME', 'ORIGIN_STATE_ABR', 'DEST_CITY_NAME', 'DEST_STATE_ABR', \n",
    "    'CRS_DEP_TIME', 'DEP_DELAY_NEW', 'CRS_ARR_TIME', 'ARR_DELAY_NEW', \n",
    "    'CANCELLED', 'CANCELLATION_CODE', 'AIR_TIME', 'DISTANCE'\n",
    "]\n",
    "\n",
    "# 3. Renommer toutes les colonnes en une seule op√©ration avec Spark\n",
    "df_spark = df_spark.toDF(*sql_compliant_column_names)\n",
    "\n",
    "# 4. Afficher le sch√©ma pour v√©rifier que tout est propre\n",
    "print(\"Sch√©ma final compatible SQL :\")\n",
    "df_spark.printSchema()\n",
    "\n",
    "# 5. Afficher les donn√©es pour validation \n",
    "df_spark.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc706c72-e6a5-4801-a95f-7c938e5f4e2a",
   "metadata": {},
   "source": [
    "**Affichage Clair des Statistiques Descriptives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6faf2996-a4b0-4df5-945e-c0cc052b448b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Statistiques Descriptives pour toutes les colonnes (Affichage Vertical) ---\n",
      "-RECORD 0--------------------------------\n",
      " summary           | count               \n",
      " YEAR              | 4316997             \n",
      " MONTH             | 4316997             \n",
      " DAY_OF_MONTH      | 4316997             \n",
      " DAY_OF_WEEK       | 4316997             \n",
      " OP_UNIQUE_CARRIER | 4316997             \n",
      " ORIGIN_CITY_NAME  | 4316997             \n",
      " ORIGIN_STATE_ABR  | 4316997             \n",
      " DEST_CITY_NAME    | 4316997             \n",
      " DEST_STATE_ABR    | 4316997             \n",
      " CRS_DEP_TIME      | 4316997             \n",
      " DEP_DELAY_NEW     | 4316997             \n",
      " CRS_ARR_TIME      | 4316997             \n",
      " ARR_DELAY_NEW     | 4316997             \n",
      " CANCELLED         | 4316997             \n",
      " CANCELLATION_CODE | 4316997             \n",
      " AIR_TIME          | 4316997             \n",
      " DISTANCE          | 4316997             \n",
      "-RECORD 1--------------------------------\n",
      " summary           | mean                \n",
      " YEAR              | 2020.0              \n",
      " MONTH             | 5.356207104151335   \n",
      " DAY_OF_MONTH      | 15.672411169153001  \n",
      " DAY_OF_WEEK       | 4.004035444083005   \n",
      " OP_UNIQUE_CARRIER | NULL                \n",
      " ORIGIN_CITY_NAME  | NULL                \n",
      " ORIGIN_STATE_ABR  | NULL                \n",
      " DEST_CITY_NAME    | NULL                \n",
      " DEST_STATE_ABR    | NULL                \n",
      " CRS_DEP_TIME      | NULL                \n",
      " DEP_DELAY_NEW     | 6.623659559556488   \n",
      " CRS_ARR_TIME      | NULL                \n",
      " ARR_DELAY_NEW     | 6.741944163059637   \n",
      " CANCELLED         | 0.06420759616001587 \n",
      " CANCELLATION_CODE | NULL                \n",
      " AIR_TIME          | 108.6959407434593   \n",
      " DISTANCE          | 777.6870062684778   \n",
      "-RECORD 2--------------------------------\n",
      " summary           | stddev              \n",
      " YEAR              | 0.0                 \n",
      " MONTH             | 3.3815256936808185  \n",
      " DAY_OF_MONTH      | 8.79893305860013    \n",
      " DAY_OF_WEEK       | 2.0052643701233133  \n",
      " OP_UNIQUE_CARRIER | NULL                \n",
      " ORIGIN_CITY_NAME  | NULL                \n",
      " ORIGIN_STATE_ABR  | NULL                \n",
      " DEST_CITY_NAME    | NULL                \n",
      " DEST_STATE_ABR    | NULL                \n",
      " CRS_DEP_TIME      | NULL                \n",
      " DEP_DELAY_NEW     | 34.71986676649214   \n",
      " CRS_ARR_TIME      | NULL                \n",
      " ARR_DELAY_NEW     | 34.10084649582124   \n",
      " CANCELLED         | 0.2451224075306195  \n",
      " CANCELLATION_CODE | NULL                \n",
      " AIR_TIME          | 66.5168443423785    \n",
      " DISTANCE          | 554.85561535912     \n",
      "-RECORD 3--------------------------------\n",
      " summary           | min                 \n",
      " YEAR              | 2020                \n",
      " MONTH             | 1                   \n",
      " DAY_OF_MONTH      | 1                   \n",
      " DAY_OF_WEEK       | 1                   \n",
      " OP_UNIQUE_CARRIER | 9E                  \n",
      " ORIGIN_CITY_NAME  | Aberdeen            \n",
      " ORIGIN_STATE_ABR  | AK                  \n",
      " DEST_CITY_NAME    | Aberdeen            \n",
      " DEST_STATE_ABR    | AK                  \n",
      " CRS_DEP_TIME      | 01:00:00            \n",
      " DEP_DELAY_NEW     | 0                   \n",
      " CRS_ARR_TIME      | 01:00:00            \n",
      " ARR_DELAY_NEW     | 0                   \n",
      " CANCELLED         | 0                   \n",
      " CANCELLATION_CODE | A                   \n",
      " AIR_TIME          | 10                  \n",
      " DISTANCE          | 29                  \n",
      "-RECORD 4--------------------------------\n",
      " summary           | max                 \n",
      " YEAR              | 2020                \n",
      " MONTH             | 11                  \n",
      " DAY_OF_MONTH      | 31                  \n",
      " DAY_OF_WEEK       | 7                   \n",
      " OP_UNIQUE_CARRIER | YX                  \n",
      " ORIGIN_CITY_NAME  | Yuma                \n",
      " ORIGIN_STATE_ABR  | WY                  \n",
      " DEST_CITY_NAME    | Yuma                \n",
      " DEST_STATE_ABR    | WY                  \n",
      " CRS_DEP_TIME      | NULL                \n",
      " DEP_DELAY_NEW     | NULL                \n",
      " CRS_ARR_TIME      | NULL                \n",
      " ARR_DELAY_NEW     | NULL                \n",
      " CANCELLED         | 1                   \n",
      " CANCELLATION_CODE | NULL                \n",
      " AIR_TIME          | NULL                \n",
      " DISTANCE          | 5095                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Statistiques Descriptives pour toutes les colonnes (Affichage Vertical) ---\")\n",
    "\n",
    "# Calculer les statistiques descriptives\n",
    "df_described = df_spark.describe()\n",
    "\n",
    "# Afficher le r√©sultat avec l'option vertical=True pour afficher les statistiques par ligne\n",
    "# Au lieu de 'df_described.show()', utilisez:\n",
    "df_described.show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed92842e-4b50-459c-a858-e33f7700dfc8",
   "metadata": {},
   "source": [
    "**Affichage Ligne par Ligne des Valeurs Manquantes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "369a11f7-53a4-4cbc-ad92-e2df33425fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîç Nombre de Valeurs Manquantes par Variable ---\n",
      "VARIABLE             COUNT NULLS\n",
      "---------------------------------\n",
      "YEAR                          0\n",
      "MONTH                         0\n",
      "DAY_OF_MONTH                  0\n",
      "DAY_OF_WEEK                   0\n",
      "OP_UNIQUE_CARRIER             0\n",
      "ORIGIN_CITY_NAME              0\n",
      "ORIGIN_STATE_ABR              0\n",
      "DEST_CITY_NAME                0\n",
      "DEST_STATE_ABR                0\n",
      "CRS_DEP_TIME                  0\n",
      "DEP_DELAY_NEW                 0\n",
      "CRS_ARR_TIME                  0\n",
      "ARR_DELAY_NEW                 0\n",
      "CANCELLED                     0\n",
      "CANCELLATION_CODE             0\n",
      "AIR_TIME                      0\n",
      "DISTANCE                      0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Calcul des valeurs manquantes \n",
    "# Cette liste cr√©e les expressions d'agr√©gation pour toutes les colonnes\n",
    "df_columns = df_spark.columns\n",
    "null_counts_expressions = [sum(col(c).isNull().cast(\"int\")).alias(f\"null_count_{c}\") for c in df_columns]\n",
    "\n",
    "# Ex√©cuter l'agr√©gation\n",
    "df_null_summary = df_spark.agg(*null_counts_expressions)\n",
    "\n",
    "# 2. Conversion et Transposition pour un affichage clair\n",
    "\n",
    "# Convertir la ligne de r√©sultat Spark en un DataFrame Pandas (plus facile √† manipuler en Python)\n",
    "# Le .collect()[0] r√©cup√®re l'unique ligne de r√©sultat\n",
    "null_counts_row = df_null_summary.collect()[0].asDict()\n",
    "\n",
    "# Transposer le dictionnaire pour obtenir une colonne de Noms et une colonne de Comptes\n",
    "# Exemple: {'null_count_YEAR': 0, 'null_count_MONTH': 0, ...}\n",
    "# devient: [('YEAR', 0), ('MONTH', 0), ...]\n",
    "\n",
    "results = []\n",
    "for key, value in null_counts_row.items():\n",
    "    # Nettoyer le nom de la colonne pour l'affichage (ex: enlever 'null_count_')\n",
    "    clean_column_name = key.replace(\"null_count_\", \"\")\n",
    "    results.append((clean_column_name, value))\n",
    "\n",
    "# 3. Affichage Ligne par Ligne des R√©sultats\n",
    "\n",
    "print(\"--- üîç Nombre de Valeurs Manquantes par Variable ---\")\n",
    "print(\"{:<20} {:>10}\".format(\"VARIABLE\", \"COUNT NULLS\"))\n",
    "print(\"-\" * 33)\n",
    "\n",
    "# Afficher chaque paire (nom de colonne, compte de NULLs) sur une ligne\n",
    "for column, count in results:\n",
    "    print(\"{:<20} {:>10}\".format(column, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1df5cb6-d95b-440f-a26b-915f0c433041",
   "metadata": {},
   "source": [
    "**Traitement des Doublons (Duplicata)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "634155d5-a0e8-4195-8153-22c42eabd7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîé V√©rification des Doublons ---\n",
      "Nombre total de lignes : 4,316,997\n",
      "Nombre de lignes distinctes : 4,316,977\n",
      "Nombre de doublons exacts trouv√©s : 20\n"
     ]
    }
   ],
   "source": [
    "# 1. Obtenir le nombre total de lignes\n",
    "total_rows = df_spark.count()\n",
    "\n",
    "# 2. Obtenir le nombre de lignes uniques\n",
    "distinct_rows = df_spark.distinct().count()\n",
    "\n",
    "# 3. Calculer le nombre de doublons\n",
    "num_duplicates = total_rows - distinct_rows\n",
    "\n",
    "print(f\"--- üîé V√©rification des Doublons ---\")\n",
    "print(f\"Nombre total de lignes : {total_rows:,}\")\n",
    "print(f\"Nombre de lignes distinctes : {distinct_rows:,}\")\n",
    "print(f\"Nombre de doublons exacts trouv√©s : {num_duplicates:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f58177-3d34-4373-8677-809b7bd1e534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ‚úÖ Suppression effectu√©e ---\n",
      "Nouveau nombre de lignes (sans doublons) : 4,316,977\n"
     ]
    }
   ],
   "source": [
    "# Suppression des Doublons:\n",
    "if num_duplicates > 0:\n",
    "    # Cr√©er un nouveau DataFrame propre sans les doublons\n",
    "    df_spark_cleaned = df_spark.dropDuplicates()\n",
    "    \n",
    "    # V√©rification (Action)\n",
    "    print(f\"\\n--- ‚úÖ Suppression effectu√©e ---\")\n",
    "    print(f\"Nouveau nombre de lignes (sans doublons) : {df_spark_cleaned.count():,}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Aucune ligne dupliqu√©e trouv√©e. Le DataFrame est d√©j√† propre.\")\n",
    "    df_spark_cleaned = df_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7000ac1-340d-4ada-8708-872bbf9e41be",
   "metadata": {},
   "source": [
    "**G√©rer les types incorrects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22c28263-279d-4f8b-b1c0-1a186eadab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üìù Sch√©ma apr√®s transformation des heures ---\n",
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY_OF_MONTH: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- OP_UNIQUE_CARRIER: string (nullable = true)\n",
      " |-- ORIGIN_CITY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_STATE_ABR: string (nullable = true)\n",
      " |-- DEST_CITY_NAME: string (nullable = true)\n",
      " |-- DEST_STATE_ABR: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: string (nullable = true)\n",
      " |-- DEP_DELAY_NEW: string (nullable = true)\n",
      " |-- CRS_ARR_TIME: string (nullable = true)\n",
      " |-- ARR_DELAY_NEW: string (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_CODE: string (nullable = true)\n",
      " |-- AIR_TIME: string (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- CRS_DEP_HOUR: integer (nullable = true)\n",
      " |-- CRS_ARR_HOUR: integer (nullable = true)\n",
      "\n",
      "\n",
      "--- üßê Aper√ßu des nouvelles colonnes ---\n",
      "+------------+------------+------------+------------+\n",
      "|CRS_DEP_TIME|CRS_DEP_HOUR|CRS_ARR_TIME|CRS_ARR_HOUR|\n",
      "+------------+------------+------------+------------+\n",
      "|    13:05:00|          13|    15:30:00|          15|\n",
      "|    18:40:00|          18|    20:20:00|          20|\n",
      "|    17:20:00|          17|    19:10:00|          19|\n",
      "|    11:05:00|          11|    13:30:00|          13|\n",
      "|    07:30:00|           7|    08:46:00|           8|\n",
      "+------------+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nous allons utiliser la fonction substring pour extraire les deux premiers caract√®res (l'heure) et cast pour \n",
    "# les convertir en IntegerType (concernant les 2 variables CRS_DEP_TIME + CRS_ARR_TIME).\n",
    "\n",
    "from pyspark.sql.functions import col, substring\n",
    "\n",
    "# Utiliser le DataFrame nettoy√© (apr√®s gestion des doublons et CANCELLATION_CODE)\n",
    "df_temp = df_spark_cleaned \n",
    "\n",
    "# Cr√©er une nouvelle colonne pour l'heure de d√©part pr√©vue (HH)\n",
    "df_temp = df_temp.withColumn(\n",
    "    \"CRS_DEP_HOUR\", \n",
    "    substring(col(\"CRS_DEP_TIME\"), 1, 2).cast(\"int\")\n",
    ")\n",
    "\n",
    "# Cr√©er une nouvelle colonne pour l'heure d'arriv√©e pr√©vue (HH)\n",
    "df_temp = df_temp.withColumn(\n",
    "    \"CRS_ARR_HOUR\", \n",
    "    substring(col(\"CRS_ARR_TIME\"), 1, 2).cast(\"int\")\n",
    ")\n",
    "\n",
    "# Remplacer le DataFrame nettoy√© final par le DataFrame avec les nouvelles colonnes d'heure\n",
    "df_spark_cleaned = df_temp\n",
    "\n",
    "# V√©rification (Action)\n",
    "print(\"--- üìù Sch√©ma apr√®s transformation des heures ---\")\n",
    "df_spark_cleaned.printSchema()\n",
    "\n",
    "print(\"\\n--- üßê Aper√ßu des nouvelles colonnes ---\")\n",
    "df_spark_cleaned.select(\"CRS_DEP_TIME\", \"CRS_DEP_HOUR\", \"CRS_ARR_TIME\", \"CRS_ARR_HOUR\").limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51f91ce7-a364-4cbf-b079-0ce4cdbee73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üìù Sch√©ma apr√®s correction de DEP_DELAY_NEW ---\n",
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY_OF_MONTH: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- OP_UNIQUE_CARRIER: string (nullable = true)\n",
      " |-- ORIGIN_CITY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_STATE_ABR: string (nullable = true)\n",
      " |-- DEST_CITY_NAME: string (nullable = true)\n",
      " |-- DEST_STATE_ABR: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: string (nullable = true)\n",
      " |-- DEP_DELAY_NEW: string (nullable = true)\n",
      " |-- CRS_ARR_TIME: string (nullable = true)\n",
      " |-- ARR_DELAY_NEW: string (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_CODE: string (nullable = true)\n",
      " |-- AIR_TIME: string (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- CRS_DEP_HOUR: integer (nullable = true)\n",
      " |-- CRS_ARR_HOUR: integer (nullable = true)\n",
      " |-- DEP_DELAY_NEW_CLEAN: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Corriger le Type de DEP_DELAY_NEW (il etait String --> Integer):\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# 1. Remplacer la cha√Æne \"NULL\" par une vraie valeur null, puis caster en IntegerType\n",
    "df_spark_cleaned = df_spark_cleaned.withColumn(\n",
    "    \"DEP_DELAY_NEW_CLEAN\",\n",
    "    when(col(\"DEP_DELAY_NEW\").cast(\"string\") == \"NULL\", None)\n",
    "    .otherwise(col(\"DEP_DELAY_NEW\"))\n",
    "    .cast(\"integer\")\n",
    ")\n",
    "\n",
    "# 2. V√©rification: Afficher le sch√©ma pour confirmer le type 'integer'\n",
    "print(\"--- üìù Sch√©ma apr√®s correction de DEP_DELAY_NEW ---\")\n",
    "df_spark_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4075be-4d81-42f5-a50a-0900cbcaf33a",
   "metadata": {},
   "source": [
    "**Transformations et actions (Analyse du Retard Moyen):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28096128-7eb0-4264-a7e2-151a0b1cf690",
   "metadata": {},
   "source": [
    "> 1. Transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4ddd54c-d917-4deb-afbf-508a4dc57490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, round, count\n",
    "# Assurez-vous d'utiliser le DataFrame avec toutes les √©tapes de nettoyage (nettoyage CANCELLATION_CODE et conversion des Timestamps)\n",
    "df_analysis = df_spark_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85eb777c-7ec3-4834-a013-a95ce7c838fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation 1 : Filter - Isoler les vols en retard (retard > 0)\n",
    "df_delays = df_analysis.filter(col(\"DEP_DELAY_NEW_CLEAN\") > 0)\n",
    "\n",
    "# Transformation 2 : Select - R√©duire les colonnes √† analyser\n",
    "df_selected = df_delays.select(\"OP_UNIQUE_CARRIER\", \"DAY_OF_WEEK\", \"DEP_DELAY_NEW_CLEAN\")\n",
    "\n",
    "# Transformation 3 & 4 : GroupBy et Agr√©gation (Calcul du Retard Moyen)\n",
    "df_carrier_day_delay = df_selected.groupBy(\"OP_UNIQUE_CARRIER\", \"DAY_OF_WEEK\").agg(\n",
    "    round(avg(col(\"DEP_DELAY_NEW_CLEAN\")), 2).alias(\"AVG_DEP_DELAY_MINUTES\"),\n",
    "    count(col(\"DEP_DELAY_NEW_CLEAN\")).alias(\"TOTAL_DELAYED_FLIGHTS\")\n",
    ")\n",
    "\n",
    "# Transformation 5 : OrderBy - Trier les r√©sultats (par les retards les plus √©lev√©s)\n",
    "df_result = df_carrier_day_delay.orderBy(col(\"AVG_DEP_DELAY_MINUTES\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca576c-145d-4894-9292-a7b80e9039ff",
   "metadata": {},
   "source": [
    "> 2. Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0820c25-14a8-4ca6-b0a1-2b7c9e338912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ü•á Top 10 des Pires Retards Moyens (Transporteur/Jour) ---\n",
      "+-----------------+-----------+---------------------+---------------------+\n",
      "|OP_UNIQUE_CARRIER|DAY_OF_WEEK|AVG_DEP_DELAY_MINUTES|TOTAL_DELAYED_FLIGHTS|\n",
      "+-----------------+-----------+---------------------+---------------------+\n",
      "|               YV|          4|                62.82|                 3542|\n",
      "|               G4|          7|                59.79|                 4068|\n",
      "|               OO|          6|                55.01|                10224|\n",
      "|               OO|          1|                 54.9|                12750|\n",
      "|               OO|          5|                54.46|                13288|\n",
      "|               OO|          4|                54.29|                12299|\n",
      "|               OH|          4|                54.22|                 5708|\n",
      "|               YV|          1|                53.72|                 3028|\n",
      "|               OO|          2|                53.71|                10346|\n",
      "|               9E|          4|                53.46|                 3986|\n",
      "+-----------------+-----------+---------------------+---------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Action 1 : Show - Afficher les 10 pires combinaisons (Retard Moyen le plus √©lev√©)\n",
    "print(\"--- ü•á Top 10 des Pires Retards Moyens (Transporteur/Jour) ---\")\n",
    "df_result.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b966ab91-d8c5-471a-81e6-8ec52871e206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nombre total de combinaisons (Transporteur/Jour de la semaine) analys√©es : 119\n"
     ]
    }
   ],
   "source": [
    "# Action 2 : Count - Compter le nombre de lignes du r√©sultat final\n",
    "total_combinations = df_result.count()\n",
    "print(f\"\\nNombre total de combinaisons (Transporteur/Jour de la semaine) analys√©es : {total_combinations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02be39c0-732c-4ac9-b763-f140f0b33bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ü§è Action 3: Take (Afficher les 5 premiers r√©sultats comme objet Python) ---\n",
      "Compagnie: YV | Jour: 4 | Retard Moyen: 62.82 min\n",
      "Compagnie: G4 | Jour: 7 | Retard Moyen: 59.79 min\n",
      "Compagnie: OO | Jour: 6 | Retard Moyen: 55.01 min\n",
      "Compagnie: OO | Jour: 1 | Retard Moyen: 54.9 min\n",
      "Compagnie: OO | Jour: 5 | Retard Moyen: 54.46 min\n",
      "\n",
      "‚úÖ Les 3 actions (show, count, take) sont compl√©t√©es.\n"
     ]
    }
   ],
   "source": [
    "# Action 3 : Take - R√©cup√©rer les 5 premi√®res lignes du r√©sultat dans une structure Python\n",
    "print(f\"\\n--- ü§è Action 3: Take (Afficher les 5 premiers r√©sultats comme objet Python) ---\")\n",
    "top_5_rows = df_result.take(5)\n",
    "\n",
    "# Affichage des 5 lignes r√©cup√©r√©es via l'action take\n",
    "for row in top_5_rows:\n",
    "    print(f\"Compagnie: {row['OP_UNIQUE_CARRIER']} | Jour: {row['DAY_OF_WEEK']} | Retard Moyen: {row['AVG_DEP_DELAY_MINUTES']} min\")\n",
    "\n",
    "print(\"\\n‚úÖ Les 3 actions (show, count, take) sont compl√©t√©es.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603629ef-19ab-4660-a1f4-938e48fd634d",
   "metadata": {},
   "source": [
    "**Analyse du dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211dca80-85bf-4a1e-9833-38476cac776a",
   "metadata": {},
   "source": [
    "> Analyse 1 : Performance Globale des Transporteurs (Q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b78691a-8e40-4725-a0dc-0f65a950b30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ü•á TOP 5 des Transporteurs les plus FIABLES (Taux d'annulation le plus bas) ---\n",
      "+-----------------+------------------------+-----------------+----------+\n",
      "|OP_UNIQUE_CARRIER|TAUX_ANNULATION_POURCENT|TOTAL_ANNULATIONS|TOTAL_VOLS|\n",
      "+-----------------+------------------------+-----------------+----------+\n",
      "|               NK|                    2.31|             2828|    122357|\n",
      "|               9E|                    3.58|             6776|    189388|\n",
      "|               AS|                    4.18|             5317|    127218|\n",
      "|               OO|                     4.5|            24416|    542338|\n",
      "|               YX|                    5.14|            10207|    198494|\n",
      "+-----------------+------------------------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "--- üëé TOP 5 des Transporteurs les moins FIABLES (Retard Moyen le plus haut) ---\n",
      "+-----------------+--------------------+--------------------+\n",
      "|OP_UNIQUE_CARRIER|RETARD_MOYEN_MINUTES|TOTAL_VOLS_EN_RETARD|\n",
      "+-----------------+--------------------+--------------------+\n",
      "|               OO|               53.68|               82485|\n",
      "|               G4|                52.6|               18954|\n",
      "|               YV|               52.33|               21123|\n",
      "|               OH|               48.03|               35501|\n",
      "|               9E|               46.87|               21876|\n",
      "+-----------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg, round, sum, desc\n",
    "\n",
    "# 1. Calcul du Taux d'Annulation (Fiabilit√©)\n",
    "# On compte le nombre total de vols annul√©s (CANCELLED = 1) par transporteur\n",
    "df_cancellation_rate = df_analysis.groupBy(\"OP_UNIQUE_CARRIER\").agg(\n",
    "    round(avg(col(\"CANCELLED\")) * 100, 2).alias(\"TAUX_ANNULATION_POURCENT\"),\n",
    "    sum(col(\"CANCELLED\")).alias(\"TOTAL_ANNULATIONS\"),\n",
    "    count(col(\"OP_UNIQUE_CARRIER\")).alias(\"TOTAL_VOLS\")\n",
    ").orderBy(col(\"TAUX_ANNULATION_POURCENT\").asc()) # Tri croissant (Moins d'annulations = Meilleur)\n",
    "\n",
    "print(\"--- ü•á TOP 5 des Transporteurs les plus FIABLES (Taux d'annulation le plus bas) ---\")\n",
    "df_cancellation_rate.show(5)\n",
    "\n",
    "# 2. Calcul du Retard Moyen (Moins de fiabilit√©)\n",
    "# Utilise DEP_DELAY_NEW_CLEAN (retards > 0 uniquement)\n",
    "df_delay_rate = df_analysis.filter(col(\"DEP_DELAY_NEW_CLEAN\") > 0).groupBy(\"OP_UNIQUE_CARRIER\").agg(\n",
    "    round(avg(col(\"DEP_DELAY_NEW_CLEAN\")), 2).alias(\"RETARD_MOYEN_MINUTES\"),\n",
    "    count(col(\"DEP_DELAY_NEW_CLEAN\")).alias(\"TOTAL_VOLS_EN_RETARD\")\n",
    ").orderBy(col(\"RETARD_MOYEN_MINUTES\").desc()) # Tri d√©croissant (Retard le plus √©lev√© = Moins Bon)\n",
    "\n",
    "print(\"--- üëé TOP 5 des Transporteurs les moins FIABLES (Retard Moyen le plus haut) ---\")\n",
    "df_delay_rate.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40f1af5-7faf-4cdf-9395-bb885b717319",
   "metadata": {},
   "source": [
    "> Analyse 2 : Distribution du Retard par Heure de D√©part (Q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "939b2c36-5cfb-4787-93b7-90da5813f576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üìù Sch√©ma et Aper√ßu apr√®s nettoyage complet des heures ---\n",
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY_OF_MONTH: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- OP_UNIQUE_CARRIER: string (nullable = true)\n",
      " |-- ORIGIN_CITY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_STATE_ABR: string (nullable = true)\n",
      " |-- DEST_CITY_NAME: string (nullable = true)\n",
      " |-- DEST_STATE_ABR: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: string (nullable = true)\n",
      " |-- DEP_DELAY_NEW: string (nullable = true)\n",
      " |-- CRS_ARR_TIME: string (nullable = true)\n",
      " |-- ARR_DELAY_NEW: string (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_CODE: string (nullable = true)\n",
      " |-- AIR_TIME: string (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- CRS_DEP_HOUR: integer (nullable = true)\n",
      " |-- CRS_ARR_HOUR: integer (nullable = true)\n",
      " |-- DEP_DELAY_NEW_CLEAN: integer (nullable = true)\n",
      " |-- CRS_DEP_TIME_CLEAN: string (nullable = true)\n",
      " |-- FLIGHT_DATE_STR: string (nullable = true)\n",
      " |-- CRS_DEP_TS_CLEAN: timestamp (nullable = true)\n",
      " |-- CRS_DEP_HOUR_CLEAN: integer (nullable = true)\n",
      "\n",
      "+------------+------------------+-------------------+------------------+\n",
      "|CRS_DEP_TIME|CRS_DEP_TIME_CLEAN|CRS_DEP_TS_CLEAN   |CRS_DEP_HOUR_CLEAN|\n",
      "+------------+------------------+-------------------+------------------+\n",
      "|13:05:00    |13:05:00          |2020-01-17 13:05:00|13                |\n",
      "|18:40:00    |18:40:00          |2020-01-17 18:40:00|18                |\n",
      "|17:20:00    |17:20:00          |2020-01-17 17:20:00|17                |\n",
      "|11:05:00    |11:05:00          |2020-01-17 11:05:00|11                |\n",
      "|07:30:00    |07:30:00          |2020-01-24 07:30:00|7                 |\n",
      "+------------+------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Resolution de probleme de valeur NULL sur colonne \"CRS_DEP_TIME\"\n",
    "\n",
    "from pyspark.sql.functions import col, concat, lpad, to_timestamp, lit, hour, when\n",
    "\n",
    "# 1. Nettoyer la colonne de temps (CRS_DEP_TIME) pour s'assurer que \"NULL\" est un vrai null\n",
    "df_analysis = df_analysis.withColumn(\n",
    "    \"CRS_DEP_TIME_CLEAN\",\n",
    "    when(col(\"CRS_DEP_TIME\") == \"NULL\", None) # G√©rer la cha√Æne \"NULL\"\n",
    "    .otherwise(col(\"CRS_DEP_TIME\"))\n",
    ")\n",
    "\n",
    "# 2. Cr√©ation de la cha√Æne de date compl√®te (n√©cessaire pour le timestamp)\n",
    "df_analysis = df_analysis.withColumn(\n",
    "    \"FLIGHT_DATE_STR\", \n",
    "    concat(\n",
    "        col(\"YEAR\"), lit(\"-\"), \n",
    "        lpad(col(\"MONTH\"), 2, \"0\"), lit(\"-\"),  \n",
    "        lpad(col(\"DAY_OF_MONTH\"), 2, \"0\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3. Cr√©ation du Timestamp de D√©part Pr√©vu (CRS_DEP_TS) en utilisant le temps nettoy√©\n",
    "df_analysis = df_analysis.withColumn(\n",
    "    \"CRS_DEP_TS_CLEAN\",\n",
    "    to_timestamp(\n",
    "        concat(col(\"FLIGHT_DATE_STR\"), lit(\" \"), col(\"CRS_DEP_TIME_CLEAN\")), \n",
    "        \"yyyy-MM-dd HH:mm:ss\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4. Cr√©er la Colonne d'Heure Propre (CRS_DEP_HOUR_CLEAN) √† partir du Timestamp\n",
    "df_analysis = df_analysis.withColumn(\n",
    "    \"CRS_DEP_HOUR_CLEAN\", \n",
    "    hour(col(\"CRS_DEP_TS_CLEAN\"))\n",
    ")\n",
    "\n",
    "# Mettre √† jour le DataFrame final\n",
    "df_spark_final_cleaned = df_analysis\n",
    "\n",
    "print(\"--- üìù Sch√©ma et Aper√ßu apr√®s nettoyage complet des heures ---\")\n",
    "df_spark_final_cleaned.printSchema()\n",
    "df_spark_final_cleaned.select(\"CRS_DEP_TIME\", \"CRS_DEP_TIME_CLEAN\", \"CRS_DEP_TS_CLEAN\", \"CRS_DEP_HOUR_CLEAN\").limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5552373-4725-4942-bc26-0b82209c3227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üìä Distribution du Retard Moyen par Heure de D√©part (Cr√©neau 0-23h) ---\n",
      "+------------------+--------------------+--------------------+\n",
      "|CRS_DEP_HOUR_CLEAN|RETARD_MOYEN_MINUTES|TOTAL_VOLS_EN_RETARD|\n",
      "+------------------+--------------------+--------------------+\n",
      "|                 1|               21.31|                 142|\n",
      "|                 2|               29.27|                 294|\n",
      "|                 3|                34.8|                 480|\n",
      "|                 4|               32.11|                 354|\n",
      "|                 5|               51.88|                7120|\n",
      "|                 6|               44.64|               29060|\n",
      "|                 7|               42.11|               34473|\n",
      "|                 8|               35.75|               40340|\n",
      "|                 9|               31.88|               43940|\n",
      "|                10|               32.62|               48114|\n",
      "|                11|               31.71|               51616|\n",
      "|                12|                33.5|               50217|\n",
      "|                13|               32.51|               50847|\n",
      "|                14|               33.44|               54549|\n",
      "|                15|               32.71|               55109|\n",
      "|                16|               34.52|               54180|\n",
      "|                17|               35.02|               60200|\n",
      "|                18|               36.03|               54355|\n",
      "|                19|               33.55|               49561|\n",
      "|                20|               36.03|               41977|\n",
      "|                21|               35.78|               21693|\n",
      "|                22|               38.92|               14171|\n",
      "|                23|               34.91|                4437|\n",
      "+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Distribution du Retard par Heure de D√©part (Q2)\n",
    "\n",
    "from pyspark.sql.functions import col, avg, round, desc, count\n",
    "\n",
    "# Filtration : Retards positifs ET heures valides\n",
    "df_hourly_delay = df_analysis.filter(\n",
    "    (col(\"DEP_DELAY_NEW_CLEAN\") > 0) & col(\"CRS_DEP_HOUR_CLEAN\").isNotNull()\n",
    ").groupBy(\"CRS_DEP_HOUR_CLEAN\").agg(\n",
    "    round(avg(col(\"DEP_DELAY_NEW_CLEAN\")), 2).alias(\"RETARD_MOYEN_MINUTES\"),\n",
    "    count(col(\"DEP_DELAY_NEW_CLEAN\")).alias(\"TOTAL_VOLS_EN_RETARD\")\n",
    ").orderBy(col(\"CRS_DEP_HOUR_CLEAN\").asc()) # Tri par l'heure de la journ√©e (0 √† 23)\n",
    "\n",
    "print(\"--- üìä Distribution du Retard Moyen par Heure de D√©part (Cr√©neau 0-23h) ---\")\n",
    "df_hourly_delay.show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1710d91c-0fa3-4666-8e05-af43f869e707",
   "metadata": {},
   "source": [
    "> Analyse 3 : Distribution des Causes d'Annulation (Q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0d19f8d-3ce3-4114-b3eb-c88407e1f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üö´ Distribution des Causes d'Annulation (Total: 277,176) ---\n",
      "+-----------------------+------------------+-------------------+\n",
      "|CANCELLATION_CODE_CLEAN|NOMBRE_ANNULATIONS|PROPORTION_POURCENT|\n",
      "+-----------------------+------------------+-------------------+\n",
      "|                      D|            236777|              85.42|\n",
      "|                      A|             18291|                6.6|\n",
      "|                      B|             17979|               6.49|\n",
      "|                      C|              4129|               1.49|\n",
      "+-----------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, desc, round, lit, when\n",
    "\n",
    "# --- √âTAPE DE NETTOYAGE : CR√âATION DE LA COLONNE CANCELLATION_CODE_CLEAN ---\n",
    "# Cette √©tape est cruciale car la colonne de code d'annulation n'est remplie que si CANCELLED = 1.\n",
    "# On s'assure que le code est propre avant l'analyse.\n",
    "\n",
    "df_analysis = df_analysis.withColumn(\n",
    "    \"CANCELLATION_CODE_CLEAN\",\n",
    "    when(col(\"CANCELLATION_CODE\").isNull() | (col(\"CANCELLATION_CODE\") == \"\"), lit(\"NOT_APPLICABLE\"))\n",
    "    .otherwise(col(\"CANCELLATION_CODE\"))\n",
    ")\n",
    "# Note : √âtant donn√© que nous filtrons sur CANCELLED = 1 ensuite, seuls les codes A, B, C, D seront conserv√©s.\n",
    "\n",
    "# --- D√âBUT DE L'ANALYSE 3 ---\n",
    "\n",
    "# Filtrer uniquement les vols annul√©s (CANCELLED = 1)\n",
    "df_cancellations_only = df_analysis.filter(col(\"CANCELLED\") == 1)\n",
    "\n",
    "# Calcul du total des annulations pour les calculs de proportions\n",
    "total_cancellations = df_cancellations_only.count()\n",
    "\n",
    "# Compter la fr√©quence de chaque code d'annulation (A, B, C, D) et calculer la proportion\n",
    "df_cause_distribution = df_cancellations_only.groupBy(\"CANCELLATION_CODE_CLEAN\").agg(\n",
    "    count(col(\"CANCELLATION_CODE_CLEAN\")).alias(\"NOMBRE_ANNULATIONS\")\n",
    ").withColumn(\n",
    "    \"PROPORTION_POURCENT\",\n",
    "    round((col(\"NOMBRE_ANNULATIONS\") / total_cancellations) * 100, 2)\n",
    ").orderBy(col(\"NOMBRE_ANNULATIONS\").desc())\n",
    "\n",
    "print(f\"--- üö´ Distribution des Causes d'Annulation (Total: {total_cancellations:,}) ---\")\n",
    "df_cause_distribution.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1447104-ae24-4c95-b497-32ea70b6473c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
